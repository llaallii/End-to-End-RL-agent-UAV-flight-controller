{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dx7ngfoll",
   "source": "# Live Training: Quadrotor Hover with SAC\n\nThis notebook trains a SAC policy on the `QuadrotorHover-v0` environment with **live visualization** of:\n- Episode reward & length curves\n- 3D quadrotor trajectory during evaluation\n- Position error convergence\n- Motor command distribution\n\n**Requirements:** `pip install -e \".[train,sim]\"` + `pip install ipykernel ipywidgets matplotlib`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gda0wm2m675",
   "source": "import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import display\nfrom matplotlib.gridspec import GridSpec\nfrom mpl_toolkits.mplot3d import Axes3D  # noqa: F401 — needed for 3D projection\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.monitor import Monitor\n\nfrom training.envs.config import HoverEnvConfig\nfrom training.scripts.train_hover import count_actor_params, make_env\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib widget\nprint(\"All imports successful.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vip10nodoh",
   "source": "## 1. Create Environment & Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oi27r6j9zs8",
   "source": "# --- Configuration ---\nTOTAL_TIMESTEPS = 200_000   # Adjust for longer/shorter training\nEVAL_INTERVAL = 2_000       # Steps between live plot updates\nSEED = 42\n\n# Load env config (uses defaults — hover at [0,0,1])\nenv_config = HoverEnvConfig.default()\nenv_config.max_episode_time = 15.0  # Shorter episodes for faster iteration\n\n# Training environment\ntrain_env = make_vec_env(\n    lambda: Monitor(make_env(env_config, seed=SEED)),\n    n_envs=1,\n    seed=SEED,\n)\n\n# Separate eval environment (no domain randomization)\neval_env = make_env(env_config, seed=SEED + 1000, eval_mode=True)\n\n# SAC with 2x128 MLP (RL007/RL008)\nmodel = SAC(\n    \"MlpPolicy\",\n    train_env,\n    policy_kwargs={\"net_arch\": [128, 128]},\n    learning_rate=3e-4,\n    batch_size=256,\n    buffer_size=100_000,\n    learning_starts=500,\n    tau=0.005,\n    gamma=0.99,\n    ent_coef=\"auto\",\n    seed=SEED,\n    verbose=0,\n)\n\nactor_params = count_actor_params(model)\nprint(f\"Actor parameters: {actor_params} (RL008 limit: 19,076)\")\nprint(f\"Observation space: {train_env.observation_space}\")\nprint(f\"Action space:      {train_env.action_space}\")\nprint(f\"Episode length:    {env_config.max_episode_time}s at {env_config.control_freq}Hz = {int(env_config.max_episode_time * env_config.control_freq)} steps\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fwmilzs8lb",
   "source": "## 2. Live Visualization Callback\n\nCustom SB3 callback that updates a 4-panel matplotlib figure during training:\n- **Top-left:** Episode reward over time\n- **Top-right:** 3D trajectory of latest eval episode\n- **Bottom-left:** Position error convergence\n- **Bottom-right:** Motor speed distribution",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8vkrn5hmpcg",
   "source": "class LivePlotCallback(BaseCallback):\n    \"\"\"SB3 callback that renders live training dashboards in Jupyter.\"\"\"\n\n    def __init__(self, eval_env, eval_freq=2000, n_eval_steps=500, verbose=0):\n        super().__init__(verbose)\n        self.eval_env = eval_env\n        self.eval_freq = eval_freq\n        self.n_eval_steps = n_eval_steps\n\n        # Accumulated data\n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_pos_errors = []\n        self.timesteps_log = []\n\n        # Latest eval trajectory\n        self.traj_positions = []\n        self.traj_motor_speeds = []\n        self.traj_goal = np.array([0, 0, 1.0])\n\n        # Figure setup\n        self.fig = plt.figure(figsize=(14, 9))\n        gs = GridSpec(2, 2, figure=self.fig, hspace=0.35, wspace=0.3)\n\n        # Panel 1: episode reward\n        self.ax_reward = self.fig.add_subplot(gs[0, 0])\n        self.ax_reward.set_xlabel(\"Timestep\")\n        self.ax_reward.set_ylabel(\"Episode Reward\")\n        self.ax_reward.set_title(\"Training Reward\")\n        self.ax_reward.grid(True, alpha=0.3)\n\n        # Panel 2: 3D trajectory\n        self.ax_3d = self.fig.add_subplot(gs[0, 1], projection=\"3d\")\n        self.ax_3d.set_xlabel(\"X (m)\")\n        self.ax_3d.set_ylabel(\"Y (m)\")\n        self.ax_3d.set_zlabel(\"Z (m)\")\n        self.ax_3d.set_title(\"Eval Trajectory\")\n\n        # Panel 3: position error\n        self.ax_pos_err = self.fig.add_subplot(gs[1, 0])\n        self.ax_pos_err.set_xlabel(\"Timestep\")\n        self.ax_pos_err.set_ylabel(\"Position Error (m)\")\n        self.ax_pos_err.set_title(\"Final Position Error\")\n        self.ax_pos_err.grid(True, alpha=0.3)\n\n        # Panel 4: motor speeds\n        self.ax_motors = self.fig.add_subplot(gs[1, 1])\n        self.ax_motors.set_xlabel(\"Step in Episode\")\n        self.ax_motors.set_ylabel(\"Motor Speed (rad/s)\")\n        self.ax_motors.set_title(\"Motor Commands (Eval)\")\n        self.ax_motors.grid(True, alpha=0.3)\n\n        self.fig.suptitle(\"Quadrotor Hover — SAC Training\", fontsize=14, fontweight=\"bold\")\n        plt.tight_layout()\n        display(self.fig)\n\n    def _on_step(self) -> bool:\n        # Collect episode stats from Monitor wrapper\n        infos = self.locals.get(\"infos\", [])\n        for info in infos:\n            ep = info.get(\"episode\")\n            if ep is not None:\n                self.ep_rewards.append(float(ep[\"r\"]))\n                self.ep_lengths.append(int(ep[\"l\"]))\n                self.timesteps_log.append(self.num_timesteps)\n                pos = info.get(\"position\")\n                goal = info.get(\"goal\")\n                if pos is not None and goal is not None:\n                    self.ep_pos_errors.append(float(np.linalg.norm(\n                        np.array(goal) - np.array(pos)\n                    )))\n\n        # Periodic evaluation + plot update\n        if self.num_timesteps % self.eval_freq == 0:\n            self._run_eval()\n            self._update_plots()\n        return True\n\n    def _run_eval(self):\n        \"\"\"Roll out the current policy on eval_env and record trajectory.\"\"\"\n        obs, info = self.eval_env.reset(seed=99)\n        self.traj_positions = [info[\"position\"].copy()]\n        self.traj_motor_speeds = []\n        self.traj_goal = info[\"goal\"].copy()\n\n        for _ in range(self.n_eval_steps):\n            action, _ = self.model.predict(obs, deterministic=True)\n            obs, _reward, terminated, truncated, info = self.eval_env.step(action)\n            self.traj_positions.append(info[\"position\"].copy())\n            self.traj_motor_speeds.append(info[\"motor_speeds\"].copy())\n            if terminated or truncated:\n                break\n\n    def _update_plots(self):\n        \"\"\"Redraw all four panels.\"\"\"\n        # --- Panel 1: Reward curve ---\n        self.ax_reward.clear()\n        self.ax_reward.set_xlabel(\"Timestep\")\n        self.ax_reward.set_ylabel(\"Episode Reward\")\n        self.ax_reward.set_title(\"Training Reward\")\n        self.ax_reward.grid(True, alpha=0.3)\n        if self.ep_rewards:\n            self.ax_reward.plot(self.timesteps_log, self.ep_rewards, \"b.\", alpha=0.3, markersize=2)\n            # Running average\n            if len(self.ep_rewards) >= 10:\n                window = min(50, len(self.ep_rewards))\n                avg = np.convolve(self.ep_rewards, np.ones(window) / window, mode=\"valid\")\n                offset = window - 1\n                self.ax_reward.plot(\n                    self.timesteps_log[offset:], avg, \"r-\", linewidth=2, label=f\"MA({window})\"\n                )\n                self.ax_reward.legend(loc=\"lower right\", fontsize=8)\n\n        # --- Panel 2: 3D trajectory ---\n        self.ax_3d.clear()\n        self.ax_3d.set_xlabel(\"X (m)\")\n        self.ax_3d.set_ylabel(\"Y (m)\")\n        self.ax_3d.set_zlabel(\"Z (m)\")\n        self.ax_3d.set_title(f\"Eval Trajectory ({len(self.traj_positions)} steps)\")\n        if self.traj_positions:\n            pos = np.array(self.traj_positions)\n            self.ax_3d.plot3D(pos[:, 0], pos[:, 1], pos[:, 2], \"b-\", linewidth=1)\n            self.ax_3d.scatter(*pos[0], color=\"green\", s=60, label=\"Start\", zorder=5)\n            self.ax_3d.scatter(*pos[-1], color=\"red\", s=60, label=\"End\", zorder=5)\n            g = self.traj_goal\n            self.ax_3d.scatter(*g, color=\"gold\", s=100, marker=\"*\", label=\"Goal\", zorder=5)\n            # Set symmetric limits around goal\n            max_range = max(\n                np.ptp(pos[:, 0]), np.ptp(pos[:, 1]), np.ptp(pos[:, 2]), 0.5\n            ) * 0.6\n            setters = [self.ax_3d.set_xlim, self.ax_3d.set_ylim, self.ax_3d.set_zlim]\n            for dim in range(3):\n                center = g[dim]\n                setters[dim](center - max_range, center + max_range)\n            self.ax_3d.legend(loc=\"upper left\", fontsize=7)\n\n        # --- Panel 3: Position error ---\n        self.ax_pos_err.clear()\n        self.ax_pos_err.set_xlabel(\"Timestep\")\n        self.ax_pos_err.set_ylabel(\"Position Error (m)\")\n        self.ax_pos_err.set_title(\"Final Position Error\")\n        self.ax_pos_err.grid(True, alpha=0.3)\n        if self.ep_pos_errors:\n            self.ax_pos_err.plot(\n                self.timesteps_log[:len(self.ep_pos_errors)],\n                self.ep_pos_errors, \"g.\", alpha=0.3, markersize=2\n            )\n            if len(self.ep_pos_errors) >= 10:\n                window = min(50, len(self.ep_pos_errors))\n                avg = np.convolve(self.ep_pos_errors, np.ones(window) / window, mode=\"valid\")\n                offset = window - 1\n                self.ax_pos_err.plot(\n                    self.timesteps_log[offset:offset + len(avg)],\n                    avg, \"darkgreen\", linewidth=2, label=f\"MA({window})\"\n                )\n                self.ax_pos_err.legend(loc=\"upper right\", fontsize=8)\n\n        # --- Panel 4: Motor speeds ---\n        self.ax_motors.clear()\n        self.ax_motors.set_xlabel(\"Step in Episode\")\n        self.ax_motors.set_ylabel(\"Motor Speed (rad/s)\")\n        self.ax_motors.set_title(\"Motor Commands (Eval)\")\n        self.ax_motors.grid(True, alpha=0.3)\n        if self.traj_motor_speeds:\n            speeds = np.array(self.traj_motor_speeds)\n            motor_labels = [\"M0 (FR,CCW)\", \"M1 (FL,CW)\", \"M2 (RL,CCW)\", \"M3 (RR,CW)\"]\n            colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"]\n            for i in range(4):\n                self.ax_motors.plot(speeds[:, i], color=colors[i], label=motor_labels[i], linewidth=1)\n            # Hover reference line\n            hover_speed = 535.6\n            self.ax_motors.axhline(hover_speed, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Hover\")\n            self.ax_motors.set_ylim(0, 850)\n            self.ax_motors.legend(loc=\"upper right\", fontsize=7, ncol=2)\n\n        self.fig.canvas.draw_idle()\n        self.fig.canvas.flush_events()\n\nprint(\"LivePlotCallback defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8cgjfcf5oj3",
   "source": "## 3. Train with Live Dashboard\n\nRun the cell below to start training. The 4-panel plot will update every `EVAL_INTERVAL` timesteps:\n\n| Panel | What it shows |\n|-------|---------------|\n| Top-left | Episode reward (blue dots) + moving average (red line) |\n| Top-right | 3D flight path from latest eval rollout (green=start, red=end, star=goal) |\n| Bottom-left | Position error at episode end — should decrease over time |\n| Bottom-right | Motor speeds during eval — should converge near ~535 rad/s (hover) |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1vnd9ivcvc1",
   "source": "live_cb = LivePlotCallback(\n    eval_env=eval_env,\n    eval_freq=EVAL_INTERVAL,\n    n_eval_steps=int(env_config.max_episode_time * env_config.control_freq),  # full episode\n)\n\nprint(f\"Starting SAC training for {TOTAL_TIMESTEPS:,} timesteps...\")\nprint(f\"Live plot updates every {EVAL_INTERVAL:,} steps.\\n\")\n\nmodel.learn(total_timesteps=TOTAL_TIMESTEPS, callback=live_cb, progress_bar=True)\n\nprint(f\"\\nTraining complete! {len(live_cb.ep_rewards)} episodes collected.\")\nif live_cb.ep_rewards:\n    last_50 = live_cb.ep_rewards[-50:]\n    print(f\"Last 50 episodes — reward: {np.mean(last_50):.1f} +/- {np.std(last_50):.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dtwvuiy16eb",
   "source": "## 4. Post-Training Evaluation\n\nRun the trained policy for a full episode and visualize the final trajectory with detailed telemetry.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "41g5onk0haa",
   "source": "# Run one full deterministic evaluation episode\nobs, info = eval_env.reset(seed=123)\npositions, velocities, eulers, motor_cmds, rewards_ep = [], [], [], [], []\ngoal = info[\"goal\"]\n\nmax_steps = int(env_config.max_episode_time * env_config.control_freq)\nfor _step in range(max_steps):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = eval_env.step(action)\n    positions.append(info[\"position\"].copy())\n    velocities.append(info[\"velocity\"].copy())\n    eulers.append(np.degrees(info[\"euler\"]))\n    motor_cmds.append(info[\"motor_speeds\"].copy())\n    rewards_ep.append(reward)\n    if terminated or truncated:\n        break\n\npositions = np.array(positions)\nvelocities = np.array(velocities)\neulers = np.array(eulers)\nmotor_cmds = np.array(motor_cmds)\nt = np.arange(len(positions)) / env_config.control_freq\n\nprint(f\"Episode: {len(positions)} steps ({t[-1]:.1f}s)\")\nprint(f\"Total reward: {sum(rewards_ep):.1f}\")\nprint(f\"Final position: {positions[-1]}\")\nprint(f\"Final pos error: {np.linalg.norm(goal - positions[-1]):.4f} m\")\nprint(f\"Terminated: {terminated}, Truncated: {truncated}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "850b9xgk9x5",
   "source": "# Detailed telemetry plots\nfig2, axes = plt.subplots(2, 3, figsize=(16, 8))\nfig2.suptitle(\"Post-Training Evaluation — Detailed Telemetry\", fontsize=14, fontweight=\"bold\")\n\n# Position XYZ\nax = axes[0, 0]\nfor i, (label, color) in enumerate(zip([\"X\", \"Y\", \"Z\"], [\"r\", \"g\", \"b\"], strict=True)):\n    ax.plot(t, positions[:, i], color=color, label=label)\n    ax.axhline(goal[i], color=color, linestyle=\"--\", alpha=0.4)\nax.set_xlabel(\"Time (s)\")\nax.set_ylabel(\"Position (m)\")\nax.set_title(\"Position vs Goal\")\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Velocity XYZ\nax = axes[0, 1]\nfor i, (label, color) in enumerate(zip([\"Vx\", \"Vy\", \"Vz\"], [\"r\", \"g\", \"b\"], strict=True)):\n    ax.plot(t, velocities[:, i], color=color, label=label)\nax.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.4)\nax.set_xlabel(\"Time (s)\")\nax.set_ylabel(\"Velocity (m/s)\")\nax.set_title(\"Velocity\")\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Euler angles\nax = axes[0, 2]\nfor i, (label, color) in enumerate(zip([\"Roll\", \"Pitch\", \"Yaw\"], [\"r\", \"g\", \"b\"], strict=True)):\n    ax.plot(t, eulers[:, i], color=color, label=label)\nax.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.4)\nax.axhline(45, color=\"red\", linestyle=\":\", alpha=0.3, label=\"Tilt limit\")\nax.axhline(-45, color=\"red\", linestyle=\":\", alpha=0.3)\nax.set_xlabel(\"Time (s)\")\nax.set_ylabel(\"Angle (deg)\")\nax.set_title(\"Attitude\")\nax.legend(fontsize=7)\nax.grid(True, alpha=0.3)\n\n# Motor speeds\nax = axes[1, 0]\ncolors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"]\nmotor_labels = [\"M0\", \"M1\", \"M2\", \"M3\"]\nfor i in range(4):\n    ax.plot(t, motor_cmds[:, i], color=colors[i], label=motor_labels[i])\nax.axhline(535.6, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Hover\")\nax.set_xlabel(\"Time (s)\")\nax.set_ylabel(\"Speed (rad/s)\")\nax.set_title(\"Motor Commands\")\nax.legend(fontsize=7)\nax.set_ylim(0, 850)\nax.grid(True, alpha=0.3)\n\n# Position error over time\nax = axes[1, 1]\npos_err = np.linalg.norm(goal - positions, axis=1)\nax.plot(t, pos_err, \"purple\", linewidth=1.5)\nax.set_xlabel(\"Time (s)\")\nax.set_ylabel(\"Error (m)\")\nax.set_title(\"Position Error\")\nax.grid(True, alpha=0.3)\n\n# 3D trajectory\nax = axes[1, 2]\nax.remove()\nax = fig2.add_subplot(2, 3, 6, projection=\"3d\")\nax.plot3D(positions[:, 0], positions[:, 1], positions[:, 2], \"b-\", linewidth=1)\nax.scatter(*positions[0], color=\"green\", s=50, label=\"Start\")\nax.scatter(*positions[-1], color=\"red\", s=50, label=\"End\")\nax.scatter(*goal, color=\"gold\", s=80, marker=\"*\", label=\"Goal\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nax.set_title(\"3D Trajectory\")\nax.legend(fontsize=7)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c8h1xoa90nf",
   "source": "## 5. Save Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lrdd5tp8ffn",
   "source": "from pathlib import Path\n\nsave_dir = Path(\"training/models\")\nsave_dir.mkdir(parents=True, exist_ok=True)\n\nmodel.save(str(save_dir / \"hover_notebook_final\"))\nprint(f\"Model saved to {save_dir / 'hover_notebook_final'}.zip\")\n\n# Cleanup\ntrain_env.close()\nprint(\"Done!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}